# Adversarial attack on classical object recognition and detection network   

## Introduction
Adversarial attack can generate attack samples for the classical object detection and recognition network. An adversarial attack in machine learning refers to a technique used to fool machine learning models. This is typically done by making subtle, calculated modifications to the input data, causing the model to make a mistake in its output or decision. It can evaluate the robustness of models, reveal vulnerabilities of systems, enhance privacy protection and act as a form of regularization to enhance generalization. 

## Demo
### Object detection

### Object recognition



## Algorithm Explanation
### Object detection
![Process](result_images/process.png)

In order to reduce the *loss* function from falling into the local optimum during the training process. We first gradient processed the *loss* function, as follows:


Here is the pseudocode:
```
Input: Original_image
Output: Adversarial_image
for ğ‘– = 1 â†’ ğ‘™ğ‘’ğ‘›(ğ‘‚ğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™_ğ‘–ğ‘šğ‘ğ‘”ğ‘’) do
  ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š â† 0
  ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜_ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š â† 0
  Ëœğ‘¥ â†ğ‘‚ğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™_ğ‘–ğ‘šğ‘ğ‘”ğ‘’[ğ‘–]
  ğ‘¥ â†ğ‘‚ğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™_ğ‘–ğ‘šğ‘ğ‘”ğ‘’[ğ‘–]
  for ğ‘— = 1 â†’ ğ‘’ğ‘ğ‘œğ‘â„ do
    if ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜_ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š < 0.02 Ã— ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š then
      ğ‘šğ‘ğ‘ ğ‘˜ â†ğ‘”ğ‘’ğ‘¡_ğ‘šğ‘ğ‘ ğ‘˜(Ëœğ‘¥,ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜_ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š)
      ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜_ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š â† ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜_ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™_ğ‘›ğ‘¢ğ‘š +ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡
    end if
    Ë†ğ‘¦ â†ğ‘Œğ‘œğ‘™ğ‘œğ‘£3(Ëœğ‘¥)
    ğ¿ğ‘œğ‘ ğ‘  â† ğ¿(Ëœğ‘¥, Ë†ğ‘¦)
    if ğ¿ğ‘œğ‘ ğ‘  > ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ and ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š > 0 then
      ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š â† âˆ’1
    end if
    if ğ¿ğ‘œğ‘ ğ‘  â‰¥ ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ then
      ğ‘Ÿğ‘’ğ‘›ğ‘’ğ‘¤_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’ â† ğ‘šğ‘ğ‘ ğ‘˜ Ã—ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’.ğ‘ğ‘™ğ‘ğ‘šğ‘_(âˆ’0.005,0.005)
      ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’ â† ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’ +ğ‘Ÿğ‘’ğ‘›ğ‘’ğ‘¤_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’
      ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’.ğ‘ğ‘™ğ‘ğ‘šğ‘_(âˆ’0.06,0.06)
      Ëœğ‘¥ â†ğ‘¥âˆ’ğ‘ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘’
      Ëœğ‘¥.ğ‘‘ğ‘ğ‘¡ğ‘.ğ‘ğ‘™ğ‘ğ‘šğ‘_(0,1)
    else
      ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š â† ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š + 1
      if ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘ ğ‘¡_ğ‘›ğ‘¢ğ‘š > 4 then
        Break
      end if
    end if
  end for
  ğ´ğ‘‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘™_ğ‘–ğ‘šğ‘ğ‘”ğ‘’[ğ‘–] â† Ëœğ‘¥
end for
```

### Object recognition

## Instruction
### Object detection
Open random_make_patch.py, set the parameters you need in the parameter list, and start running.

* **Model loading**: First, the required model will be loaded from the pretrained_models_pytorch/pretrainedmodels folder. If it is target recognition, the model parameters of the corresponding network will be downloaded through the Internet. If it is target detection, the model will be loaded in yolo.py in the folder. and return.

* **Sample loading**: After the model is loaded, the Dataloader will load all images and labels from the corresponding folder where the training data is stored.

* **Training process**: Use the loaded images and models in random_make_patch.py to train adversarial samples. The finally generated adversarial samples, original images, original prediction images, and post-attack prediction images are stored in the advspec folder at the same level. The results generated by the attack will be saved to predict_results_without_attack.json.

* **Evaluate**: Use the coco toolbox to compare the saved json and the original new.json and generate the pr curve and store it in prcurve at the same level.
### Object recognition

* **Model loading**: Open the test_deepfool.py or test_CWattack file, read the original sample and target sample in the images, and input them into the network to obtain the category of the original sample and the category of the target sample. Use the category of the target sample as the target of targeted confrontation.
  
* **Training process**: Use deepfool and CWattack perturb the sample, and the output result includes a picture composed of the perturbation.

* **Evaluate**: After perturbation, the picture, the number of iterations, the original category, the target category, the category of the interfered image, and put the interfered image into the corresponding out file.
